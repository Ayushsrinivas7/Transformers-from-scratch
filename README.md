# ðŸš€ **AyushSrinivas7 (Prathipati Yadu Vamsi) - IIT Patna**  

## ðŸŽ¯ **Project: Building the Transformer from Scratch**  
_(Inspired by the "Attention Is All You Need" Paper)_  

I implemented the **entire Transformer architecture from scratch**, following the principles outlined in the **"Attention Is All You Need"** paper.  

ðŸ”¹ **Key Highlights:**  
âœ… Followed **Umar Jamil's video series** for guidance.  
âœ… Wrote **clean, well-structured code** with **detailed explanations** covering:  
   - **What** the Transformer does  
   - **Why** each component is essential  
   - **How** each mechanism works  

ðŸ”— **[See My Implementation](https://drive.google.com/uc?export=view&id=1c1cOeIYYzpOOasmUvFC0gV5wDcK9Hpqh)**  

ðŸ“„ **Also attaching the original research paper for reference.**  

---

This project is a deep dive into **self-attention, multi-head attention, and sequence modeling** from the ground up! ðŸš€  
Let me know if you need any refinements. ðŸ”¥  
